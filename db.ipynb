{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone-client in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (5.0.1)\n",
      "Requirement already satisfied: cohere in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (5.9.2)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from pinecone-client) (2024.8.30)\n",
      "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from pinecone-client) (1.1.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from pinecone-client) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from pinecone-client) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from pinecone-client) (2.2.3)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.34.0 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from cohere) (1.35.19)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from cohere) (1.9.7)\n",
      "Requirement already satisfied: httpx>=0.21.2 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from cohere) (0.27.2)\n",
      "Requirement already satisfied: httpx-sse==0.4.0 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from cohere) (0.4.0)\n",
      "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from cohere) (0.9.0)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from cohere) (2.9.1)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from cohere) (2.23.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from cohere) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from cohere) (0.20.0)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from cohere) (2.32.0.20240914)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.19 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere) (1.35.19)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere) (0.10.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from httpx>=0.21.2->cohere) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from httpx>=0.21.2->cohere) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from httpx>=0.21.2->cohere) (3.9)\n",
      "Requirement already satisfied: sniffio in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from tokenizers<1,>=0.15->cohere) (0.24.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from tqdm>=4.64.1->pinecone-client) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from botocore<1.36.0,>=1.35.19->boto3<2.0.0,>=1.34.0->cohere) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\onedrive\\desktop\\assignment_submission\\.venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.19->boto3<2.0.0,>=1.34.0->cohere) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pinecone-client cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Load PDF\n",
    "from PyPDF2 import PdfFileReader\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from io import BytesIO\n",
    "# import fitz  # PyMuPDF\n",
    "\n",
    "def load_pdf(pdf_path):\n",
    "    # with open(pdf_path, \"rb\") as file:\n",
    "    pdf = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text() or \"\"  # Extract text from each page\n",
    "        # print(\"extracted text \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Get Embeddings\n",
    "import cohere\n",
    "\n",
    "co = cohere.Client('brV6x6dombNlECLz3031MhwM2UxHom1Gx2mp0BX8')\n",
    "\n",
    "def get_embeddings(text):\n",
    "    response = co.embed(texts=[text])\n",
    "    embeddings = response.embeddings[0]\n",
    "    print(\"Dimensions of the embeddings:\", len(embeddings))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone client\n",
    "# pinecone_client = pinecone.Client(api_key=\"564a623c-4d0a-4247-8f53-1e171c26b4e2\")\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key='564a623c-4d0a-4247-8f53-1e171c26b4e2')\n",
    "# index_name = \"quickstart\"\n",
    "\n",
    "# from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "\n",
    "# pc = Pinecone(api_key=\"564a623c-4d0a-4247-8f53-1e171c26b4e2\")\n",
    "# index = pc.Index(\"example-index\")\n",
    "# index = pinecone_client.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=\"564a623c-4d0a-4247-8f53-1e171c26b4e2\")\n",
    "index_name = \"quickstart2\"\n",
    "\n",
    "# # if not pc.has_index(index_name):\n",
    "# pc.create_index(\n",
    "#     name=index_name,\n",
    "#     dimension=4096,\n",
    "#     metric=\"cosine\",\n",
    "#     spec=ServerlessSpec(\n",
    "#         cloud='aws', \n",
    "#         region='us-east-1'\n",
    "#     ) \n",
    "#     ) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc.delete_index('quickstart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(id, embedding, text):\n",
    "    # Add the text to metadata while storing the embeddings\n",
    "    index.upsert([(id, embedding, {\"text\": text})])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query_embedding):\n",
    "    results = index.query(vector=query_embedding, top_k=5,\n",
    "                          include_values=True,      # Include vector values in the response\n",
    "                          include_metadata=True )   # Include metadata in the response\n",
    "    return results['matches']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, context):\n",
    "    response = co.generate(prompt=f\"Question: {question}\\nContext: {context}\\nAnswer:\", max_tokens=100)\n",
    "    return response.generations[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the text into sections of 150 words\n",
    "def split_into_sections(text, section_size=150):\n",
    "    words = text.split()\n",
    "    sections = [' '.join(words[i:i+section_size]) for i in range(0, len(words), section_size)]\n",
    "    return sections\n",
    "\n",
    "# Modify the store_embeddings function to store multiple sections\n",
    "def store_embeddings_in_sections(document_id, document_text, section_size=150):\n",
    "    sections = split_into_sections(document_text, section_size)\n",
    "    \n",
    "    for idx, section in enumerate(sections):\n",
    "        section_id = f\"{document_id}_section_{idx+1}\"  # Create a unique ID for each section\n",
    "        section_embedding = get_embeddings(section)  # Get the embedding for the section\n",
    "        index.upsert([(section_id, section_embedding, {'text': section})])  # Store with metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the embeddings: 4096\n",
      "Dimensions of the embeddings: 4096\n",
      "Dimensions of the embeddings: 4096\n"
     ]
    }
   ],
   "source": [
    "# Example workflow\n",
    "query = \"What is the main topic of the document?\"\n",
    "document_text = load_pdf('my_pdf.pdf')\n",
    "\n",
    "# Store the document's sections in Pinecone\n",
    "store_embeddings_in_sections(\"document_id\", document_text, section_size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the embeddings: 4096\n",
      "Section of text : interact with the bot. ● Documentation on how the user can upload files, ask questions, and view the bot's responses. ● Example interactions demonstrating the bot's capabilities. Guidelines: ● Use Docker to containerize the application for easy deployment. ● Ensure the system can handle large documents and multiple queries without significant performance drops. ● Share the code, deployment instructions, and the final working model through GitHub. General Guidelines: 1. Ensure modular and scalable code following best practices for both frontend and backend development. 2. Document your approach thoroughly, explaining your decisions, challenges faced, and solutions. 3. Provide a detailed ReadMe file in your GitHub repository, including setup and usage instructions. 4. Submissions should include: ○ Source code for both the notebook and the interface. ○ A fully functional Colab notebook. ○ Documentation on the pipeline and deployment instructions. generative responses are created. ● Provide several example queries and the corresponding outputs. Part 2: Interactive QA Bot Interface Problem Statement: Develop an interactive interface for the QA bot from Part 1, allowing users to input queries and retrieve answers in real time. The interface should enable users to upload documents and ask questions based on the content of the uploaded document. Task Requirements: 1. Build a simple frontend interface using Streamlit or Gradio , allowing users to upload PDF documents and ask questions. 2. Integrate the backend from Part 1 to process the PDF, store document embeddings, and provide real-time answers to user queries. 3. Ensure that the system can handle multiple queries efficiently and provide accurate, contextually relevant responses. 4. Allow users to see the retrieved document segments alongside the generated answer. Deliverables: ● A deployed QA bot with a frontend interface where users can upload documents and Gen AI Engineer / Machine Learning Engineer Assignment Part 1: Retrieval-Augmented Generation (RAG) Model for QA Bot Problem Statement: Develop a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA) bot for a business. Use a vector database like Pinecone DB and a generative model like Cohere API (or any other available alternative). The QA bot should be able to retrieve relevant information from a dataset and generate coherent answers. Task Requirements: 1. Implement a RAG-based model that can handle questions related to a provided document or dataset. 2. Use a vector database (such as Pinecone ) to store and retrieve document embeddings efficiently. 3. Test the model with several queries and show how well it retrieves and generates accurate answers from the document. Deliverables: ● A Colab notebook demonstrating the entire pipeline, from data loading to question answering. ● Documentation explaining the model architecture, approach to retrieval, and how\n",
      "Answer:  To effectively utilize Docker in this context, you can follow these key steps:\n",
      "\n",
      "1. Containerization: Package your application into Docker containers, encapsulating both the backend and frontend components. This ensures consistency across deployments and facilitates scalability.\n",
      "\n",
      "2. Docker File Creation: Develop a Docker file detailing the dependencies and configurations required for the application's proper functioning. This file serves as a blueprint for building the container.\n",
      "\n",
      "3. Image Publication: Build the Docker image using the Docker file and optionally\n"
     ]
    }
   ],
   "source": [
    "query = \"how to use docker in this required context ? keep it brisk\"\n",
    "query_embedding = get_embeddings(query)\n",
    "retrieved_segments = retrieve_documents(query_embedding)\n",
    "# print(retrieved_segments)\n",
    "# print(document_text)\n",
    "# Assuming retrieved_segments is a list of segments\n",
    "context = \" \".join([segment['metadata'].get('text', '') for segment in retrieved_segments])\n",
    "\n",
    "# Print the context to verify\n",
    "print(\"Section of text :\", context)\n",
    "\n",
    "\n",
    "# Generate an answer using the query and context\n",
    "answer = generate_answer(query, context)\n",
    "print(\"Answer:\", answer)\n",
    "# print(context)\n",
    "# answer = generate_answer(query, context)\n",
    "# print(\"Answer:\", answer)\n",
    "# for segment in retrieved_segments:\n",
    "#     print(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
